#!/usr/bin/env python3
"""AI Dataset Radar v5 - Competitive Intelligence System.

Main entry point for the competitive intelligence workflow.
Integrates HuggingFace, GitHub, and Blog monitoring.
"""

import argparse
import json
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path

import yaml

# Add src to path
src_dir = Path(__file__).parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from utils.logging_config import get_logger, setup_logging

logger = get_logger("main_intel")

from trackers.org_tracker import OrgTracker
from trackers.github_tracker import GitHubTracker
from trackers.blog_tracker import BlogTracker
from analyzers.data_type_classifier import DataTypeClassifier, DataType
from analyzers.paper_filter import PaperFilter
from intel_report import IntelReportGenerator
from scrapers.arxiv import ArxivScraper
from scrapers.hf_papers import HFPapersScraper
from scrapers.huggingface import HuggingFaceScraper
from output_formatter import DualOutputFormatter


def load_config(config_path: str = "config.yaml") -> dict:
    """Load configuration from YAML file.

    Args:
        config_path: Path to the YAML config file.

    Returns:
        Configuration dictionary, or empty dict if file is invalid.
    """
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
            return config if config else {}
    except FileNotFoundError:
        logger.warning("Config file not found: %s, using defaults", config_path)
        return {}
    except yaml.YAMLError as e:
        logger.warning("Invalid YAML in %s: %s, using defaults", config_path, e)
        return {}



def format_insights_prompt(
    all_datasets: list,
    blog_activity: list,
    github_activity: list,
    papers: list,
    datasets_by_type: dict,
    lab_activity: dict = None,
    vendor_activity: dict = None,
) -> str:
    """Format data with analysis prompt for LLM consumption.

    This output is designed to be read by Claude Code / Claude App,
    which will then perform the analysis using its native LLM capabilities.
    Surfaces all available intelligence data with full context.
    """
    lines = []
    lines.append("\n" + "=" * 60)
    lines.append("  AI Dataset Radar - ç«žäº‰æƒ…æŠ¥åˆ†æžææ–™")
    lines.append("=" * 60 + "\n")

    # â”€â”€ Section 1: Lab Activity (org-by-org with datasets AND models) â”€â”€
    lines.append("## ä¸€ã€AI Labs åŠ¨æ€ï¼ˆæŒ‰ç»„ç»‡ï¼‰\n")
    labs = (lab_activity or {}).get("labs", {})
    has_lab_activity = False

    category_names = {
        "frontier_labs": "Frontier Labsï¼ˆä¸€çº¿å®žéªŒå®¤ï¼‰",
        "emerging_labs": "Emerging Labsï¼ˆæ–°å…´å®žéªŒå®¤ï¼‰",
        "research_labs": "Research Labsï¼ˆç ”ç©¶æœºæž„ï¼‰",
        "china_opensource": "ä¸­å›½å¼€æºå¤§æ¨¡åž‹",
        "china_closedsource": "ä¸­å›½é—­æºå¤§æ¨¡åž‹",
    }

    for cat_key, cat_display in category_names.items():
        cat_data = labs.get(cat_key, {})
        # Filter to orgs with actual activity
        active_orgs = {
            k: v for k, v in cat_data.items()
            if v.get("datasets") or v.get("models")
        }
        if not active_orgs:
            continue

        has_lab_activity = True
        lines.append(f"### {cat_display}\n")

        for org_name, org_data in active_orgs.items():
            org_display = org_name.replace("_", " ").title()
            ds_list = org_data.get("datasets", [])
            model_list = org_data.get("models", [])
            lines.append(f"**{org_display}** â€” {len(ds_list)} æ•°æ®é›†, {len(model_list)} æ¨¡åž‹")

            # Datasets with full info
            for ds in ds_list:
                ds_id = ds.get("id", "")
                downloads = ds.get("downloads", 0)
                likes = ds.get("likes", 0)
                desc = ds.get("description", "")
                # Clean up description whitespace
                if desc:
                    desc = " ".join(desc.split())[:300]
                lines.append(f"- ðŸ“¦ **{ds_id}** (downloads: {downloads:,}, likes: {likes})")
                if desc:
                    lines.append(f"  {desc}")
                # Show meaningful tags (filter out noise)
                tags = ds.get("tags", [])
                meaningful_tags = [
                    t for t in tags
                    if not t.startswith(("region:", "library:", "size_categories:",
                                        "format:", "arxiv:", "language:"))
                    and t not in ("region:us",)
                ][:8]
                if meaningful_tags:
                    lines.append(f"  æ ‡ç­¾: {', '.join(meaningful_tags)}")

            # Models with context - show top models by downloads+likes, limit noise
            notable_models = [m for m in model_list if m.get("downloads", 0) > 0 or m.get("likes", 0) > 0]
            if not notable_models:
                # All models are zero-activity, just summarize
                if model_list:
                    sample = model_list[0].get("id", "").split("/")[-1] if model_list else ""
                    lines.append(f"- ðŸ¤– *{len(model_list)} ä¸ªæ¨¡åž‹ï¼ˆå‡æ— ä¸‹è½½/ç‚¹èµžï¼Œå¦‚ {sample} ç­‰ï¼‰*")
                model_list_to_show = []
            else:
                top_models = sorted(notable_models, key=lambda m: -(m.get("downloads", 0) + m.get("likes", 0) * 100))
                model_list_to_show = top_models[:5]
            for model in model_list_to_show:
                model_id = model.get("id", "")
                downloads = model.get("downloads", 0)
                likes = model.get("likes", 0)
                pipeline = model.get("pipeline_tag", "")
                model_tags = model.get("tags", [])
                # Extract meaningful tags for models
                meaningful = [
                    t for t in model_tags
                    if not t.startswith(("region:", "base_model:", "endpoints_",
                                        "license:", "arxiv:"))
                    and t not in ("safetensors", "transformers", "pytorch", "en",
                                  "model_hub_mixin", "pytorch_model_hub_mixin")
                ][:6]
                lines.append(f"- ðŸ¤– **{model_id}** (downloads: {downloads:,}, likes: {likes}, pipeline: {pipeline})")
                if meaningful:
                    lines.append(f"  æ ‡ç­¾: {', '.join(meaningful)}")
            if len(notable_models) > 5:
                lines.append(f"  *(å¦æœ‰ {len(notable_models) - 5} ä¸ªæ¨¡åž‹çœç•¥)*")

            lines.append("")

    if not has_lab_activity:
        lines.append("*æœ¬å‘¨æ—  AI Labs æ–°æ´»åŠ¨*\n")

    # â”€â”€ Section 2: Vendor Activity â”€â”€
    lines.append("## äºŒã€æ•°æ®ä¾›åº”å•†åŠ¨æ€ï¼ˆç«žå“ï¼‰\n")
    vendors = (vendor_activity or {}).get("vendors", {})
    has_vendor_activity = False

    for tier_name, tier_data in vendors.items():
        active_vendors = {
            k: v for k, v in tier_data.items()
            if v.get("datasets") or v.get("models")
        }
        if not active_vendors:
            continue

        has_vendor_activity = True
        lines.append(f"### {tier_name.replace('_', ' ').title()}\n")

        for vendor_name, vendor_data in active_vendors.items():
            vendor_display = vendor_name.replace("_", " ").title()
            ds_list = vendor_data.get("datasets", [])
            model_list = vendor_data.get("models", [])
            lines.append(f"**{vendor_display}** â€” {len(ds_list)} æ•°æ®é›†, {len(model_list)} æ¨¡åž‹")

            for ds in ds_list:
                ds_id = ds.get("id", "")
                downloads = ds.get("downloads", 0)
                desc = ds.get("description", "")
                if desc:
                    desc = " ".join(desc.split())[:300]
                lines.append(f"- ðŸ“¦ **{ds_id}** (downloads: {downloads:,})")
                if desc:
                    lines.append(f"  {desc}")
            lines.append("")

    if not has_vendor_activity:
        lines.append("*æœ¬å‘¨æ— ä¾›åº”å•† HuggingFace æ–°æ´»åŠ¨*\n")

    # â”€â”€ Section 3: Dataset Classification Results â”€â”€
    lines.append("## ä¸‰ã€æ•°æ®é›†åˆ†ç±»åˆ†æž\n")
    if datasets_by_type:
        # Show classified types first, "other" last
        classified = {k: v for k, v in datasets_by_type.items()
                      if (k.value if hasattr(k, 'value') else str(k)) != "other" and v}
        other = {k: v for k, v in datasets_by_type.items()
                 if (k.value if hasattr(k, 'value') else str(k)) == "other" and v}

        total = sum(len(v) for v in datasets_by_type.values())
        classified_count = sum(len(v) for v in classified.values())
        lines.append(f"å…± {total} ä¸ªæ•°æ®é›†ï¼Œå·²åˆ†ç±» {classified_count} ä¸ªï¼š\n")

        for dtype, ds_list in classified.items():
            type_name = dtype.value if hasattr(dtype, 'value') else str(dtype)
            lines.append(f"- **{type_name}**: {len(ds_list)} ä¸ª â€” {', '.join(ds.get('id', '') for ds in ds_list[:5])}")

        if other:
            other_list = list(other.values())[0]
            lines.append(f"- **æœªåˆ†ç±»**: {len(other_list)} ä¸ª â€” {', '.join(ds.get('id', '') for ds in other_list[:5])}")
        lines.append("")
    else:
        lines.append("*æ— åˆ†ç±»æ•°æ®*\n")

    # â”€â”€ Section 4: Blog Activity (full titles, more articles) â”€â”€
    lines.append("## å››ã€åšå®¢è¦é—»\n")
    if blog_activity:
        active_blogs = [b for b in blog_activity if b.get("articles")]
        if active_blogs:
            for blog in active_blogs:
                source = blog.get("source", "æœªçŸ¥")
                articles = blog.get("articles", [])[:5]
                if articles:
                    lines.append(f"### {source}")
                    for art in articles:
                        title = art.get("title", "æ— æ ‡é¢˜")
                        url = art.get("url", "")
                        summary = art.get("summary", "")
                        if summary:
                            summary = " ".join(summary.split())[:200]
                        lines.append(f"- [{title}]({url})")
                        if summary:
                            lines.append(f"  {summary}")
                    lines.append("")
        else:
            lines.append("*æ— åšå®¢æ›´æ–°*\n")
    else:
        lines.append("*æ— åšå®¢æ›´æ–°*\n")

    # â”€â”€ Section 5: GitHub Activity (high + medium relevance) â”€â”€
    lines.append("## äº”ã€GitHub æ´»åŠ¨\n")
    if github_activity:
        # Collect all repos with relevance info
        all_repos = []
        for org in github_activity:
            org_name = org.get("org", "")
            for repo in org.get("repos_updated", []):
                repo_copy = dict(repo)
                repo_copy["org"] = org_name
                all_repos.append(repo_copy)

        # High relevance
        high = [r for r in all_repos if r.get("relevance") == "high"]
        high = sorted(high, key=lambda x: -x.get("stars", 0))
        # Medium relevance
        medium = [r for r in all_repos if r.get("relevance") == "medium"]
        medium = sorted(medium, key=lambda x: -x.get("stars", 0))[:10]

        if high:
            lines.append(f"### é«˜ç›¸å…³ ({len(high)} ä¸ª)")
            for repo in high:
                lines.append(f"- **{repo.get('org')}/{repo.get('name')}** â­ {repo.get('stars', 0)}")
                if repo.get("description"):
                    lines.append(f"  {repo.get('description', '')[:120]}")
                signals = repo.get("signals", [])
                if signals:
                    lines.append(f"  ä¿¡å·: {', '.join(str(s) for s in signals[:5])}")
            lines.append("")

        if medium:
            lines.append(f"### ä¸­ç›¸å…³ (Top {len(medium)})")
            for repo in medium:
                lines.append(f"- **{repo.get('org')}/{repo.get('name')}** â­ {repo.get('stars', 0)}")
                if repo.get("description"):
                    lines.append(f"  {repo.get('description', '')[:120]}")
            lines.append("")

        # Summary stats
        total_repos = len(all_repos)
        active_orgs = len([o for o in github_activity if o.get("repos_updated")])
        lines.append(f"*å…±ç›‘æŽ§ {active_orgs} ä¸ªç»„ç»‡ï¼Œ{total_repos} ä¸ªæ´»è·ƒä»“åº“*\n")
    else:
        lines.append("*æ—  GitHub æ´»åŠ¨*\n")

    # â”€â”€ Section 6: Papers (full titles, longer abstracts) â”€â”€
    lines.append("## å…­ã€ç›¸å…³è®ºæ–‡\n")
    if papers:
        # Group by category if available
        by_cat = {}
        for paper in papers:
            cat = paper.get("category", "å…¶ä»–")
            if cat not in by_cat:
                by_cat[cat] = []
            by_cat[cat].append(paper)

        for cat, paper_list in by_cat.items():
            if len(by_cat) > 1:
                lines.append(f"### {cat}\n")
            for paper in paper_list[:8]:
                title = paper.get("title", "æ— æ ‡é¢˜")
                source = paper.get("source", "")
                url = paper.get("url", "")
                abstract = paper.get("abstract", "")
                if abstract:
                    abstract = " ".join(abstract.split())[:400]
                matched_kw = paper.get("_matched_keywords", [])

                link_str = f"[{source}]({url})" if url else f"[{source}]"
                lines.append(f"- **{title}** {link_str}")
                if matched_kw:
                    lines.append(f"  å…³é”®è¯å‘½ä¸­: {', '.join(matched_kw[:5])}")
                if abstract:
                    lines.append(f"  æ‘˜è¦: {abstract}")
            lines.append("")
    else:
        lines.append("*æ— ç›¸å…³è®ºæ–‡*\n")

    # â”€â”€ Analysis Prompt â”€â”€
    lines.append("=" * 60)
    lines.append("  åˆ†æžè¦æ±‚")
    lines.append("=" * 60 + "\n")
    lines.append("""èƒŒæ™¯ï¼šä½ æ˜¯ AI è®­ç»ƒæ•°æ®è¡Œä¸šçš„ç«žäº‰æƒ…æŠ¥åˆ†æžå¸ˆã€‚è¯»è€…æ˜¯ä¸€å®¶æ•°æ®æœåŠ¡å…¬å¸çš„ç®¡ç†å±‚ï¼Œéœ€è¦ä»Žä»¥ä¸Šæ•°æ®ä¸­èŽ·å–å¯æ‰§è¡Œçš„å•†ä¸šæ´žå¯Ÿã€‚

è¯·æä¾›ä»¥ä¸‹åˆ†æžï¼š

### 1. å…³é”®å‘çŽ°ï¼ˆKey Findingsï¼‰
- æœ¬å‘¨æœ€å€¼å¾—å…³æ³¨çš„ 3-5 ä¸ªäº‹ä»¶ï¼ˆæ•°æ®é›†å‘å¸ƒã€æ¨¡åž‹åŠ¨æ€ã€å·¥å…·æ›´æ–°ï¼‰ï¼Œé€æ¡è¯´æ˜ŽåŽŸå› å’Œå•†ä¸šæ„ä¹‰
- ç‰¹åˆ«å…³æ³¨ï¼šæ–°å‘å¸ƒçš„é«˜ä»·å€¼è®­ç»ƒæ•°æ®é›†ã€RLHF/å¯¹é½ç›¸å…³åŠ¨æ€ã€åˆæˆæ•°æ®æ–¹å‘

### 2. ç»„ç»‡åŠ¨æ€å›¾è°±
- å„ AI Lab æœ¬å‘¨çš„æ•°æ®ç­–ç•¥åŠ¨å‘ï¼ˆå‘äº†ä»€ä¹ˆæ•°æ®é›†ï¼Ÿè®­ç»ƒäº†ä»€ä¹ˆæ¨¡åž‹ï¼Ÿæ¨¡åž‹éœ€è¦ä»€ä¹ˆç±»åž‹çš„æ•°æ®ï¼Ÿï¼‰
- æ•°æ®ä¾›åº”å•†ç«žå“çš„æœ€æ–°åŠ¨ä½œï¼ˆäº§å“å‘å¸ƒã€å¼€æºå·¥å…·ã€æŠ€æœ¯åšå®¢ä¼ é€’çš„ä¿¡å·ï¼‰
- ä¸­å›½ vs æµ·å¤– AI Labs çš„æ•°æ®å¸ƒå±€å·®å¼‚

### 3. æ•°æ®éœ€æ±‚ä¿¡å·
- ä»Žæ¨¡åž‹å‘å¸ƒåæŽ¨ï¼šå“ªäº›ç±»åž‹çš„è®­ç»ƒæ•°æ®éœ€æ±‚åœ¨ä¸Šå‡ï¼Ÿï¼ˆå¦‚ RLHFã€å¤šæ¨¡æ€ã€ä»£ç ã€Agent ç­‰ï¼‰
- ä»Žè®ºæ–‡æ–¹å‘çœ‹ï¼šå­¦æœ¯ç•Œåœ¨æŽ¢ç´¢ä»€ä¹ˆæ–°çš„æ•°æ®æ–¹æ³•è®ºï¼Ÿï¼ˆå¦‚æ–°çš„æ ‡æ³¨èŒƒå¼ã€åˆæˆæ•°æ®æŠ€æœ¯ã€æ•°æ®è´¨é‡è¯„ä¼°ï¼‰
- ä»Žåšå®¢å’Œ GitHub çœ‹ï¼šæ•°æ®å·¥å…·é“¾æœ‰ä»€ä¹ˆæ–°è¶‹åŠ¿ï¼Ÿ

### 4. è¡ŒåŠ¨å»ºè®®
- é’ˆå¯¹æ•°æ®æœåŠ¡å…¬å¸ï¼Œæœ¬å‘¨æœ‰å“ªäº›å€¼å¾—è·Ÿè¿›çš„æœºä¼šï¼Ÿ
- æœ‰å“ªäº›å€¼å¾—è­¦æƒ•çš„ç«žäº‰å¨èƒï¼Ÿ
- å»ºè®®ä¼˜å…ˆå…³æ³¨çš„æ•°æ®ç±»åž‹æˆ–æŠ€æœ¯æ–¹å‘

### 5. å¼‚å¸¸ä¸Žå¾…æŽ’æŸ¥
- æ•°æ®é‡‡é›†ä¸­æ˜¯å¦æœ‰å¼‚å¸¸ï¼ˆå¦‚æŸæ•°æ®æºè¿”å›ž 0 ç»“æžœã€åˆ†ç±»è¦†ç›–çŽ‡è¿‡ä½Žç­‰ï¼‰
- å€¼å¾—äººå·¥å¤æŸ¥çš„æ¡ç›®

è¯·ç”¨ä¸­æ–‡å›žç­”ã€‚åˆ†æžåº”è¯¥å…·ä½“ã€å¯æ‰§è¡Œï¼Œé¿å…æ³›æ³›è€Œè°ˆã€‚å¼•ç”¨å…·ä½“çš„æ•°æ®é›†åç§°ã€ç»„ç»‡åç§°å’Œè®ºæ–‡æ ‡é¢˜ã€‚
""")

    return "\n".join(lines)


def validate_config(config: dict) -> list[str]:
    """Validate configuration has required sections.

    Args:
        config: Configuration dictionary.

    Returns:
        List of warning messages.
    """
    warnings = []

    if not config:
        warnings.append("Configuration is empty, using defaults")
        return warnings

    # Check for watched orgs
    watched_orgs = config.get("watched_orgs", {})
    if not watched_orgs:
        warnings.append("No watched_orgs configured - no HuggingFace orgs will be tracked")

    # Check for watched vendors
    watched_vendors = config.get("watched_vendors", {})
    if not watched_vendors:
        warnings.append("No watched_vendors configured - no vendors will be tracked")

    # Check for blogs
    blogs = watched_vendors.get("blogs", [])
    if not blogs:
        warnings.append("No blogs configured - blog tracking disabled")

    return warnings


def fetch_dataset_readmes(datasets: list[dict], hf_scraper: HuggingFaceScraper) -> list[dict]:
    """Fetch README content for datasets to improve classification.

    Uses parallel fetching with rate-limited workers for speed.

    Args:
        datasets: List of datasets.
        hf_scraper: HuggingFace scraper instance.

    Returns:
        Datasets with card_data populated.
    """
    logger.info("Fetching dataset READMEs for better classification...")
    to_fetch = [
        (i, ds) for i, ds in enumerate(datasets[:30])
        if ds.get("id") and not ds.get("card_data")
    ]

    if not to_fetch:
        return datasets

    def _fetch_one(idx_ds):
        idx, ds = idx_ds
        ds_id = ds.get("id", "")
        try:
            card_data = hf_scraper.fetch_dataset_readme(ds_id)
            return idx, card_data
        except Exception as e:
            logger.warning("Could not fetch README for %s: %s", ds_id, e)
            return idx, None

    count = 0
    with ThreadPoolExecutor(max_workers=5) as executor:
        for idx, card_data in executor.map(_fetch_one, to_fetch):
            if card_data:
                datasets[idx]["card_data"] = card_data[:5000]
                count += 1

    logger.info("Fetched %d READMEs", count)
    return datasets


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="AI Dataset Radar v5 - Competitive Intelligence System"
    )
    parser.add_argument(
        "--config",
        default="config.yaml",
        help="Path to configuration file",
    )
    parser.add_argument(
        "--days",
        type=int,
        default=7,
        help="Look back period in days (default: 7)",
    )
    parser.add_argument(
        "--output",
        default=None,
        help="Output file path (default: data/intel_report_DATE.md)",
    )
    parser.add_argument(
        "--json",
        action="store_true",
        help="Also save raw data as JSON",
    )
    parser.add_argument(
        "--no-labs",
        action="store_true",
        help="Skip AI labs tracking",
    )
    parser.add_argument(
        "--no-vendors",
        action="store_true",
        help="Skip vendor tracking",
    )
    parser.add_argument(
        "--no-github",
        action="store_true",
        help="Skip GitHub tracking",
    )
    parser.add_argument(
        "--no-blogs",
        action="store_true",
        help="Skip blog tracking",
    )
    parser.add_argument(
        "--no-papers",
        action="store_true",
        help="Skip paper fetching",
    )
    parser.add_argument(
        "--no-readme",
        action="store_true",
        help="Skip fetching dataset READMEs",
    )
    parser.add_argument(
        "--no-insights",
        action="store_true",
        help="Skip LLM analysis prompt output (enabled by default)",
    )

    args = parser.parse_args()

    # Set up logging based on verbosity
    setup_logging(level="INFO")

    # Load config
    logger.info("=" * 60)
    logger.info("  AI Dataset Radar v5")
    logger.info("  Competitive Intelligence System")
    logger.info("=" * 60)

    config = load_config(args.config)

    # Initialize components
    org_tracker = OrgTracker(config)
    github_tracker = GitHubTracker(config)
    blog_tracker = BlogTracker(config)
    data_classifier = DataTypeClassifier(config)
    paper_filter = PaperFilter(config)
    report_generator = IntelReportGenerator(config)
    hf_scraper = HuggingFaceScraper(config)

    # 1-3. Fetch all data sources in parallel for maximum speed
    lab_activity = {"labs": {}}
    vendor_activity = {"vendors": {}}
    github_activity = []
    blog_activity = []
    papers = []

    # Pre-build paper scrapers so they're ready for parallel submission
    arxiv_scraper = None
    hf_papers_scraper = None
    if not args.no_papers:
        arxiv_config = config.get("sources", {}).get("arxiv", {})
        if arxiv_config.get("enabled", True):
            arxiv_scraper = ArxivScraper(limit=50, config=config)
        hf_config = config.get("sources", {}).get("hf_papers", {})
        if hf_config.get("enabled", True):
            hf_papers_scraper = HFPapersScraper(
                limit=50,
                days=hf_config.get("days", 7),
            )

    futures = {}
    with ThreadPoolExecutor(max_workers=6, thread_name_prefix="radar") as executor:
        if not args.no_labs:
            logger.info("Tracking AI labs on HuggingFace...")
            futures["labs"] = executor.submit(org_tracker.fetch_lab_activity, days=args.days)

        if not args.no_vendors:
            logger.info("Tracking data vendors on HuggingFace...")
            futures["vendors"] = executor.submit(org_tracker.fetch_vendor_activity, days=args.days)

        if not args.no_github:
            logger.info("Tracking GitHub organizations...")
            futures["github"] = executor.submit(github_tracker.fetch_all_orgs, days=args.days)

        if not args.no_blogs:
            logger.info("Tracking company blogs...")
            futures["blogs"] = executor.submit(blog_tracker.fetch_all_blogs, days=args.days)

        if arxiv_scraper:
            logger.info("Fetching from arXiv...")
            futures["arxiv"] = executor.submit(arxiv_scraper.fetch)

        if hf_papers_scraper:
            logger.info("Fetching from HuggingFace Papers...")
            futures["hf_papers"] = executor.submit(hf_papers_scraper.fetch)

        # Collect results as they complete
        for key, future in futures.items():
            try:
                result = future.result()
                if key == "labs":
                    lab_activity = {"labs": result}
                elif key == "vendors":
                    vendor_activity = {"vendors": result}
                elif key == "github":
                    github_activity = result.get("vendors", []) + result.get("labs", [])
                    active_count = sum(1 for a in github_activity if a.get("repos_updated"))
                    repo_count = sum(len(a.get("repos_updated", [])) for a in github_activity)
                    logger.info("Found %d active orgs with %d updated repos", active_count, repo_count)
                elif key == "blogs":
                    blog_activity = result
                    active_count = sum(1 for a in blog_activity if a.get("articles"))
                    article_count = sum(len(a.get("articles", [])) for a in blog_activity)
                    logger.info("Found %d active blogs with %d relevant articles", active_count, article_count)
                elif key == "arxiv":
                    logger.info("Found %d arXiv papers", len(result))
                    papers.extend(paper_filter.filter_papers(result))
                    logger.info("Relevant arXiv: %d", len(papers))
                elif key == "hf_papers":
                    logger.info("Found %d HF papers", len(result))
                    filtered = paper_filter.filter_papers(result)
                    papers.extend(filtered)
                    logger.info("Relevant HF papers: %d", len(filtered))
            except Exception as e:
                logger.warning("Error fetching %s: %s", key, e)

    # 4. Collect all datasets for classification
    all_datasets = []

    # From labs
    for category in lab_activity.get("labs", {}).values():
        for org_data in category.values():
            all_datasets.extend(org_data.get("datasets", []))

    # From vendors
    for tier in vendor_activity.get("vendors", {}).values():
        for vendor_data in tier.values():
            all_datasets.extend(vendor_data.get("datasets", []))

    logger.info("Collected %d datasets from tracked organizations", len(all_datasets))

    # 5. Fetch dataset READMEs for better classification
    if not args.no_readme and all_datasets:
        all_datasets = fetch_dataset_readmes(all_datasets, hf_scraper)

    # 6. Classify datasets
    logger.info("Classifying datasets by training type...")
    datasets_by_type = data_classifier.group_by_type(all_datasets)

    summary = data_classifier.summarize(all_datasets)
    logger.info("Classified datasets: %d/%d relevant", summary['relevant'], summary['total'])
    logger.info("Other ratio: %.1f%%", summary['other_ratio'] * 100)
    for dtype, count in summary["by_type"].items():
        if count > 0:
            logger.info("  %s: %d", dtype, count)

    # 7. Papers already fetched in parallel above (arXiv + HF Papers)

    # 8. Generate report
    logger.info("Generating intelligence report...")

    report = report_generator.generate(
        lab_activity=lab_activity,
        vendor_activity=vendor_activity,
        datasets_by_type=datasets_by_type,
        papers=papers,
        github_activity=github_activity,
        blog_activity=blog_activity,
    )

    # Prepare structured data for JSON output
    datasets_json = {}
    for dtype, ds_list in datasets_by_type.items():
        key = dtype.value if isinstance(dtype, DataType) else str(dtype)
        datasets_json[key] = [
            {k: v for k, v in ds.items() if not k.startswith("_")}
            for ds in ds_list
        ]

    all_data = {
        "period": {
            "days": args.days,
            "start": None,
            "end": datetime.now().isoformat(),
        },
        "labs_activity": lab_activity,
        "vendor_activity": vendor_activity,
        "github_activity": github_activity,
        "blog_posts": blog_activity,
        "datasets": all_datasets,
        "datasets_by_type": datasets_json,
        "papers": papers,
    }

    # Determine output directory and save reports
    output_dir = Path(config.get("report", {}).get("output_dir", "data"))
    output_dir.mkdir(parents=True, exist_ok=True)

    # Initialize dual formatter
    formatter = DualOutputFormatter(output_dir=str(output_dir / "reports"))

    # Use custom output path if specified
    if args.output:
        output_path = Path(args.output)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report)
        logger.info("Report saved to: %s", output_path)

        if args.json:
            json_path = output_path.with_suffix(".json")
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(
                    formatter._format_json_output(all_data),
                    f, ensure_ascii=False, indent=2, default=str
                )
            logger.info("JSON data saved to: %s", json_path)
    else:
        # Use DualOutputFormatter for default path
        md_path, json_path = formatter.save_reports(
            markdown_content=report,
            data=all_data,
            filename_prefix="intel_report"
        )
        logger.info("Report saved to: %s", md_path)
        logger.info("JSON data saved to: %s", json_path)

    # Print console summary
    logger.info(report_generator.generate_console_summary(
        lab_activity, vendor_activity, datasets_by_type,
        github_activity, blog_activity
    ))

    logger.info("Done!")

    # Output insights prompt for LLM analysis (Claude Code / Claude App)
    if not args.no_insights:
        insights_content = format_insights_prompt(
            all_datasets=all_datasets,
            blog_activity=blog_activity,
            github_activity=github_activity,
            papers=papers,
            datasets_by_type=datasets_by_type,
            lab_activity=lab_activity,
            vendor_activity=vendor_activity,
        )
        print(insights_content)

        # Save insights prompt to file for reference
        insights_prompt_path = output_dir / "reports" / f"intel_report_{datetime.now().strftime('%Y-%m-%d')}_insights_prompt.md"
        with open(insights_prompt_path, "w", encoding="utf-8") as f:
            f.write(insights_content)
        logger.info("Insights prompt saved to: %s", insights_prompt_path)
        logger.info("")
        logger.info(">>> AI åˆ†æžå®ŒæˆåŽï¼Œè¯·å°†åˆ†æžç»“æžœä¿å­˜åˆ°:")
        logger.info(">>> %s", str(insights_prompt_path).replace("_prompt.md", ".md"))


if __name__ == "__main__":
    main()
