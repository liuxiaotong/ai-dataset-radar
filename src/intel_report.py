"""Intelligence report generator v5 for competitive analysis.

Generates structured markdown reports with:
- US AI Labs activity (HuggingFace)
- Data vendor monitoring (GitHub + Blogs + HuggingFace)
- Datasets categorized by training type (enhanced classifier)
- Papers filtered and categorized by topic
"""

from datetime import datetime
from typing import Optional

from analyzers.data_type_classifier import DataType, DataTypeClassifier
from trackers.blog_tracker import map_blog_to_vendor


class IntelReportGenerator:
    """Generate competitive intelligence reports v5."""

    def __init__(self, config: dict):
        """Initialize the report generator.

        Args:
            config: Configuration dict.
        """
        self.config = config
        report_config = config.get("report", {})
        self.limits = report_config.get("limits", {})

    def generate(
        self,
        lab_activity: dict,
        vendor_activity: dict,
        datasets_by_type: dict,
        papers: list[dict],
        github_activity: list[dict] = None,
        blog_activity: list[dict] = None,
    ) -> str:
        """Generate the full intelligence report.

        Args:
            lab_activity: Activity from AI labs.
            vendor_activity: Activity from data vendors.
            datasets_by_type: Datasets grouped by training type.
            papers: Relevant research papers.
            github_activity: GitHub organization activities.
            blog_activity: Blog/RSS activities.

        Returns:
            Markdown formatted report.
        """
        lines = []

        # Header
        lines.append("# AI æ•°æ®æƒ…æŠ¥å‘¨æŠ¥")
        lines.append(f"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}*")
        lines.append("")

        # Executive Summary
        lines.extend(self._generate_summary(
            lab_activity, vendor_activity, datasets_by_type,
            github_activity, blog_activity
        ))

        # US AI Labs Activity
        lines.extend(self._generate_labs_section(lab_activity))

        # Data Vendor Activity (enhanced with GitHub + Blogs)
        lines.extend(self._generate_vendors_section(
            vendor_activity, github_activity, blog_activity
        ))

        # Datasets by Type
        lines.extend(self._generate_datasets_section(datasets_by_type))

        # Papers (categorized)
        lines.extend(self._generate_papers_section(papers))

        # Footer
        lines.append("---")
        lines.append("> Report generated by AI Dataset Radar v5 â€” Competitive Intelligence System")

        return "\n".join(lines)

    def _generate_summary(
        self,
        lab_activity: dict,
        vendor_activity: dict,
        datasets_by_type: dict,
        github_activity: list[dict] = None,
        blog_activity: list[dict] = None,
    ) -> list[str]:
        """Generate executive summary section."""
        lines = []
        lines.append("## ğŸ“Š æœ¬å‘¨æ‘˜è¦")
        lines.append("")

        # Count labs with activity
        active_labs = 0
        lab_datasets = 0
        for category in lab_activity.get("labs", {}).values():
            for org_data in category.values():
                if org_data.get("datasets") or org_data.get("models"):
                    active_labs += 1
                    lab_datasets += len(org_data.get("datasets", []))

        # Count vendor activities
        active_vendors = 0
        vendor_datasets = 0
        for tier in vendor_activity.get("vendors", {}).values():
            for vendor_data in tier.values():
                if vendor_data.get("datasets"):
                    active_vendors += 1
                    vendor_datasets += len(vendor_data.get("datasets", []))

        # GitHub activity
        github_repos = 0
        github_orgs = 0
        if github_activity:
            for activity in github_activity:
                if activity.get("repos_updated"):
                    github_orgs += 1
                    github_repos += len(activity["repos_updated"])

        # Blog activity
        blog_articles = 0
        blog_sources = 0
        if blog_activity:
            for activity in blog_activity:
                if activity.get("articles"):
                    blog_sources += 1
                    blog_articles += len(activity["articles"])

        # Count by type
        type_counts = {}
        total_datasets = 0
        other_count = 0

        for dtype, datasets in datasets_by_type.items():
            count = len(datasets)
            total_datasets += count
            # Handle both DataType enum and string keys
            key = dtype.value if isinstance(dtype, DataType) else str(dtype)
            type_counts[key] = count
            if dtype == DataType.OTHER or key == "other":
                other_count = count

        # Calculate other ratio
        other_ratio = other_count / total_datasets if total_datasets > 0 else 0

        lines.append(f"- **æ´»è·ƒ AI Labs**: {active_labs} å®¶ï¼Œå‘å¸ƒ {lab_datasets} ä¸ªæ•°æ®é›†")
        lines.append(f"- **æ•°æ®ä¾›åº”å•†ç›‘æ§**: HF({active_vendors}å®¶), GitHub({github_orgs}ç»„ç»‡/{github_repos}ä»“åº“), åšå®¢({blog_sources}æº/{blog_articles}æ–‡ç« )")
        lines.append(f"- **é«˜ä»·å€¼æ•°æ®é›†**: {total_datasets} ä¸ªï¼ˆå·²åˆ†ç±» {total_datasets - other_count} ä¸ªï¼‰")

        if type_counts:
            # Sort by count, exclude "other" from top display
            sorted_types = sorted(
                [(k, v) for k, v in type_counts.items() if k != "other"],
                key=lambda x: x[1],
                reverse=True
            )[:4]
            if sorted_types:
                top_str = ", ".join(f"{dtype}({count})" for dtype, count in sorted_types)
                lines.append(f"- **çƒ­é—¨ç±»å‹**: {top_str}")

        # Warning if too many unclassified
        if other_ratio > 0.3:
            lines.append(f"- **âš ï¸ åˆ†ç±»è¦†ç›–ç‡**: {1-other_ratio:.0%}ï¼ˆ{other_count}ä¸ªæœªåˆ†ç±»ï¼‰")

        lines.append("")
        return lines

    def _generate_labs_section(self, lab_activity: dict) -> list[str]:
        """Generate US AI Labs section."""
        lines = []
        lines.append("## ğŸ”¬ ç¾å›½ AI Labs åŠ¨æ€")
        lines.append("")

        labs = lab_activity.get("labs", {})
        limit = self.limits.get("labs_per_category", 10)

        # Frontier Labs
        frontier = labs.get("frontier_labs", {})
        if frontier:
            lines.append("### Frontier Labsï¼ˆä¸€çº¿å®éªŒå®¤ï¼‰")
            lines.append("")
            lines.append("| æœºæ„ | æœ¬å‘¨æ•°æ®é›† | æœ¬å‘¨æ¨¡å‹ | ä»£è¡¨å†…å®¹ |")
            lines.append("|------|-----------|---------|---------|")

            for org_name, org_data in list(frontier.items())[:limit]:
                ds_count = len(org_data.get("datasets", []))
                model_count = len(org_data.get("models", []))

                rep = ""
                if org_data.get("datasets"):
                    ds = org_data["datasets"][0]
                    rep = ds.get("id", "").split("/")[-1][:30]
                elif org_data.get("models"):
                    model = org_data["models"][0]
                    rep = model.get("id", "").split("/")[-1][:30]

                org_display = org_name.replace("_", " ").title()
                lines.append(f"| {org_display} | {ds_count} | {model_count} | {rep} |")

            lines.append("")

            # Details for frontier labs with datasets
            for org_name, org_data in list(frontier.items())[:5]:
                if org_data.get("datasets"):
                    org_display = org_name.replace("_", " ").title()
                    lines.append(f"**{org_display}** æ•°æ®é›†:")
                    for ds in org_data["datasets"][:3]:
                        ds_id = ds.get("id", "Unknown")
                        ds_name = ds_id.split("/")[-1] if "/" in ds_id else ds_id
                        downloads = ds.get("downloads", 0)
                        lines.append(f"- [{ds_name}](https://huggingface.co/datasets/{ds_id}) ({downloads:,} downloads)")
                    lines.append("")

        # Emerging Labs
        emerging = labs.get("emerging_labs", {})
        if emerging:
            lines.append("### Emerging Labsï¼ˆæ–°å…´å®éªŒå®¤ï¼‰")
            lines.append("")
            lines.append("| æœºæ„ | æ•°æ®é›† | æ¨¡å‹ | ä»£è¡¨å†…å®¹ |")
            lines.append("|------|--------|------|---------|")

            for org_name, org_data in list(emerging.items())[:limit]:
                ds_count = len(org_data.get("datasets", []))
                model_count = len(org_data.get("models", []))
                rep = ""
                if org_data.get("datasets"):
                    rep = org_data["datasets"][0].get("id", "").split("/")[-1][:30]

                org_display = org_name.replace("_", " ").title()
                lines.append(f"| {org_display} | {ds_count} | {model_count} | {rep} |")

            lines.append("")

        # Research Labs
        research = labs.get("research_labs", {})
        if research:
            lines.append("### Research Labsï¼ˆç ”ç©¶æœºæ„ï¼‰")
            lines.append("")
            lines.append("| æœºæ„ | æ•°æ®é›† | æ¨¡å‹ | ä»£è¡¨å†…å®¹ |")
            lines.append("|------|--------|------|---------|")

            for org_name, org_data in list(research.items())[:limit]:
                ds_count = len(org_data.get("datasets", []))
                model_count = len(org_data.get("models", []))
                rep = ""
                if org_data.get("datasets"):
                    rep = org_data["datasets"][0].get("id", "").split("/")[-1][:30]

                org_display = org_name.replace("_", " ").title()
                lines.append(f"| {org_display} | {ds_count} | {model_count} | {rep} |")

            lines.append("")

        if not frontier and not emerging and not research:
            lines.append("*æœ¬å‘¨æ— ç›‘æ§ç›®æ ‡çš„æ–°æ´»åŠ¨*")
            lines.append("")

        return lines

    def _generate_vendors_section(
        self,
        vendor_activity: dict,
        github_activity: list[dict] = None,
        blog_activity: list[dict] = None,
    ) -> list[str]:
        """Generate data vendor section with GitHub + Blog data."""
        lines = []
        lines.append("## ğŸ¢ æ•°æ®ä¾›åº”å•†åŠ¨æ€ï¼ˆç«å“ç›‘æ§ï¼‰")
        lines.append("")

        # Aggregate all vendor activities by vendor name
        vendors = {}

        # HuggingFace datasets
        hf_vendors = vendor_activity.get("vendors", {})
        for tier_data in hf_vendors.values():
            for vendor_name, vendor_data in tier_data.items():
                if vendor_name not in vendors:
                    vendors[vendor_name] = {"datasets": [], "github": [], "blog": []}
                vendors[vendor_name]["datasets"].extend(vendor_data.get("datasets", []))

        # GitHub activity
        if github_activity:
            for activity in github_activity:
                org = activity.get("org", "")
                # Normalize org name
                vendor_key = org.lower().replace("-", "_")
                if vendor_key not in vendors:
                    vendors[vendor_key] = {"datasets": [], "github": [], "blog": []}
                vendors[vendor_key]["github"].extend(activity.get("repos_updated", []))

        # Blog activity
        if blog_activity:
            for activity in blog_activity:
                source = activity.get("source", "")
                vendor_key = map_blog_to_vendor(source)
                if vendor_key not in vendors:
                    vendors[vendor_key] = {"datasets": [], "github": [], "blog": []}
                vendors[vendor_key]["blog"].extend(activity.get("articles", []))

        # Check if any vendor has activity
        has_activity = any(
            v["datasets"] or v["github"] or v["blog"]
            for v in vendors.values()
        )

        if not has_activity:
            lines.append("*æœ¬å‘¨æ— ç›‘æ§ä¾›åº”å•†çš„å…¬å¼€åŠ¨æ€*")
            lines.append("")
            return lines

        # Generate sections for each vendor with activity
        for vendor_name, activity in sorted(vendors.items()):
            if not any([activity["datasets"], activity["github"], activity["blog"]]):
                continue

            vendor_display = vendor_name.replace("_", " ").title()
            lines.append(f"### {vendor_display}")
            lines.append("")

            # Blog articles
            if activity["blog"]:
                lines.append("**ğŸ“° åšå®¢åŠ¨æ€**")
                for article in activity["blog"][:3]:
                    title = article.get("title", "")[:60]
                    url = article.get("url", "")
                    summary = article.get("summary", "")[:80]
                    if summary:
                        summary = f" - {summary}..."
                    lines.append(f"- [{title}]({url}){summary}")
                lines.append("")

            # GitHub repos
            if activity["github"]:
                lines.append("**ğŸ’» GitHub æ´»åŠ¨**")
                for repo in activity["github"][:3]:
                    name = repo.get("name", "")
                    desc = repo.get("description", "No description")[:60]
                    url = repo.get("url", "")
                    signals = repo.get("signals", [])
                    stars = repo.get("stars", 0)

                    lines.append(f"- [`{name}`]({url}) â­{stars}")
                    lines.append(f"  {desc}")
                    if signals:
                        lines.append(f"  ä¿¡å·: {', '.join(signals[:3])}")
                lines.append("")

            # HuggingFace datasets
            if activity["datasets"]:
                lines.append("**ğŸ“¦ æ•°æ®é›†å‘å¸ƒ**")
                for ds in activity["datasets"][:3]:
                    ds_id = ds.get("id", "Unknown")
                    ds_name = ds_id.split("/")[-1] if "/" in ds_id else ds_id
                    downloads = ds.get("downloads", 0)
                    lines.append(f"- [{ds_name}](https://huggingface.co/datasets/{ds_id}) ({downloads:,} downloads)")
                lines.append("")

        return lines

    def _generate_datasets_section(self, datasets_by_type: dict) -> list[str]:
        """Generate datasets by type section with DataType enum support."""
        lines = []
        lines.append("## ğŸ“Š é«˜ä»·å€¼æ•°æ®é›†ï¼ˆæŒ‰ç±»å‹ï¼‰")
        lines.append("")

        # Calculate other ratio for warning
        total = sum(len(ds) for ds in datasets_by_type.values())
        other_key = DataType.OTHER
        other_count = len(datasets_by_type.get(other_key, []))
        # Also check string key
        if not other_count:
            other_count = len(datasets_by_type.get("other", []))

        other_ratio = other_count / total if total > 0 else 0

        if other_ratio > 0.3:
            lines.append(f"> âš ï¸ æ³¨æ„ï¼š{other_ratio:.0%} çš„æ•°æ®é›†æœªèƒ½åˆ†ç±»ï¼Œå»ºè®®æ£€æŸ¥åˆ†ç±»è§„åˆ™")
            lines.append("")

        limit = self.limits.get("datasets_per_type", 10)

        # Get ordered types
        ordered_types = DataTypeClassifier.get_ordered_types()

        for dtype, display_name in ordered_types:
            # Check both enum and string key
            datasets = datasets_by_type.get(dtype, [])
            if not datasets:
                datasets = datasets_by_type.get(dtype.value, [])

            if not datasets:
                continue

            lines.append(f"### {display_name}")
            lines.append("")
            lines.append("| æ•°æ®é›† | å‘å¸ƒè€… | ä¸‹è½½é‡ | ä¿¡å· |")
            lines.append("|--------|--------|--------|------|")

            for ds in datasets[:limit]:
                ds_id = ds.get("id", ds.get("name", "Unknown"))
                ds_name = ds_id.split("/")[-1] if "/" in ds_id else ds_id
                author = ds_id.split("/")[0] if "/" in ds_id else "-"
                downloads = ds.get("downloads", 0)

                # Get signals
                signals = ds.get("signals", ["-"])
                if isinstance(signals, list):
                    signal_str = ", ".join(str(s) for s in signals[:3])
                else:
                    signal_str = str(signals)

                lines.append(
                    f"| [{ds_name}](https://huggingface.co/datasets/{ds_id}) | "
                    f"{author} | {downloads:,} | {signal_str} |"
                )

            lines.append("")

        if not any(datasets_by_type.values()):
            lines.append("*æœ¬å‘¨æ— æ–°çš„é«˜ä»·å€¼æ•°æ®é›†*")
            lines.append("")

        return lines

    def _generate_papers_section(self, papers: list[dict]) -> list[str]:
        """Generate papers section with categories."""
        lines = []
        lines.append("## ğŸ“„ ç›¸å…³è®ºæ–‡ï¼ˆæ•°æ®æ–¹æ³•è®ºï¼‰")
        lines.append("")

        if not papers:
            lines.append("*æœ¬å‘¨æ— é«˜åº¦ç›¸å…³è®ºæ–‡*")
            lines.append("")
            return lines

        # Group by category
        by_category = {}
        for paper in papers:
            cat = paper.get("category", "å…¶ä»–ç›¸å…³")
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(paper)

        category_order = ["RLHF/åå¥½å­¦ä¹ ", "æ•°æ®é›†æ„å»º", "æ ‡æ³¨æ–¹æ³•è®º", "æŒ‡ä»¤å¾®è°ƒ", "å…¶ä»–ç›¸å…³"]

        for cat in category_order:
            if cat not in by_category or not by_category[cat]:
                continue

            lines.append(f"### {cat}")
            lines.append("")
            lines.append("| è®ºæ–‡ | è¦ç‚¹ | é“¾æ¥ |")
            lines.append("|------|------|------|")

            for paper in by_category[cat][:5]:
                title = paper.get("title", "Unknown")[:55]
                if len(paper.get("title", "")) > 55:
                    title += "..."

                # Get highlight
                highlight = paper.get("highlight", "")
                if not highlight:
                    # Extract from matched keywords
                    keywords = paper.get("_matched_keywords", [])[:2]
                    highlight = ", ".join(keywords) if keywords else "-"

                # Get link
                url = paper.get("url", "")
                if url:
                    link = f"[arXiv]({url})"
                else:
                    link = "-"

                lines.append(f"| {title} | {highlight} | {link} |")

            lines.append("")

        return lines

    def generate_console_summary(
        self,
        lab_activity: dict,
        vendor_activity: dict,
        datasets_by_type: dict,
        github_activity: list[dict] = None,
        blog_activity: list[dict] = None,
    ) -> str:
        """Generate a brief console summary."""
        lines = []
        lines.append("=" * 60)
        lines.append("  AI Dataset Radar v5 - ç«äº‰æƒ…æŠ¥æ‘˜è¦")
        lines.append("=" * 60)
        lines.append("")

        # Labs summary
        labs = lab_activity.get("labs", {})
        for category, category_name in [
            ("frontier_labs", "Frontier Labs"),
            ("emerging_labs", "Emerging Labs"),
            ("research_labs", "Research Labs"),
        ]:
            cat_data = labs.get(category, {})
            active = [k for k, v in cat_data.items() if v.get("datasets") or v.get("models")]
            if active:
                lines.append(f"  {category_name}: {', '.join(active[:5])}")

        # GitHub summary
        if github_activity:
            active_orgs = [a["org"] for a in github_activity if a.get("repos_updated")]
            if active_orgs:
                lines.append(f"  GitHubæ´»è·ƒ: {', '.join(active_orgs[:5])}")

        # Blog summary
        if blog_activity:
            active_blogs = [a["source"] for a in blog_activity if a.get("articles")]
            if active_blogs:
                lines.append(f"  åšå®¢æ›´æ–°: {', '.join(active_blogs[:5])}")

        # Type summary
        lines.append("")
        lines.append("  æ•°æ®é›†ç±»å‹åˆ†å¸ƒ:")
        for dtype, datasets in datasets_by_type.items():
            if datasets:
                key = dtype.value if isinstance(dtype, DataType) else str(dtype)
                lines.append(f"    {key}: {len(datasets)}")

        lines.append("")
        lines.append("=" * 60)

        return "\n".join(lines)
